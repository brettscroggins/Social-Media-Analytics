{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hony_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates('image_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>post</th>\n",
       "      <th>no_likes</th>\n",
       "      <th>no_comments</th>\n",
       "      <th>image_labels</th>\n",
       "      <th>image_text</th>\n",
       "      <th>web_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...</td>\n",
       "      <td>“I didn’t get accepted into any of the univers...</td>\n",
       "      <td>493.6k</td>\n",
       "      <td>3,806</td>\n",
       "      <td>footwear fashion accessory jeans shoulder shoe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casa Adela Brandon Stanton New York City Bingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...</td>\n",
       "      <td>\"I'm trying to live my life without conflict s...</td>\n",
       "      <td>268k</td>\n",
       "      <td>2,714</td>\n",
       "      <td>sitting vehicle temple headgear street health ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brandon Stanton New York City India Black Pant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/158...</td>\n",
       "      <td>“I was a full time housewife. I kept mostly to...</td>\n",
       "      <td>411.7k</td>\n",
       "      <td>4,103</td>\n",
       "      <td>woman facial expression lady smile snapshot gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Felines of New York: A Glimpse I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/053...</td>\n",
       "      <td>\"I don't know how old I am.\" (Mumbai, India)</td>\n",
       "      <td>451.4k</td>\n",
       "      <td>5,207</td>\n",
       "      <td>face facial expression yellow head smile eye t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India Humans of New York Humans of New York: S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/808...</td>\n",
       "      <td>“I resented my mother for the longest time. Sh...</td>\n",
       "      <td>237.2k</td>\n",
       "      <td>1,201</td>\n",
       "      <td>hand finger nail arm jewellery ring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Sadhana H. Varshney Mumbai Human...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...   \n",
       "1  https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...   \n",
       "2  https://instagram.fftw1-1.fna.fbcdn.net/vp/158...   \n",
       "3  https://instagram.fftw1-1.fna.fbcdn.net/vp/053...   \n",
       "4  https://instagram.fftw1-1.fna.fbcdn.net/vp/808...   \n",
       "\n",
       "                                                post no_likes no_comments  \\\n",
       "0  “I didn’t get accepted into any of the univers...   493.6k       3,806   \n",
       "1  \"I'm trying to live my life without conflict s...     268k       2,714   \n",
       "2  “I was a full time housewife. I kept mostly to...   411.7k       4,103   \n",
       "3       \"I don't know how old I am.\" (Mumbai, India)   451.4k       5,207   \n",
       "4  “I resented my mother for the longest time. Sh...   237.2k       1,201   \n",
       "\n",
       "                                        image_labels image_text  \\\n",
       "0  footwear fashion accessory jeans shoulder shoe...        NaN   \n",
       "1  sitting vehicle temple headgear street health ...        NaN   \n",
       "2  woman facial expression lady smile snapshot gi...        NaN   \n",
       "3  face facial expression yellow head smile eye t...        NaN   \n",
       "4                hand finger nail arm jewellery ring        NaN   \n",
       "\n",
       "                                        web_entities  \n",
       "0  Casa Adela Brandon Stanton New York City Bingh...  \n",
       "1  Brandon Stanton New York City India Black Pant...  \n",
       "2  New York City Felines of New York: A Glimpse I...  \n",
       "3  India Humans of New York Humans of New York: S...  \n",
       "4  New York City Sadhana H. Varshney Mumbai Human...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(x):\n",
    "    pattern = \"\\(\\D*\\)\" # non-digits within parentheses (excludes instances with multiple posts, e.g. \"Caption...(2/3)\")\n",
    "    match = re.findall(pattern, x)\n",
    "    if len(match) == 0:\n",
    "        return (\"NYC, USA\")\n",
    "    else:\n",
    "        return match[len(match)-1].lstrip(\"(\").rstrip(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NYC, USA                   326\n",
       "Mumbai, India               29\n",
       "Jaipur, India               27\n",
       "São Paulo, Brazil           25\n",
       "Rio de Janeiro, Brazil      20\n",
       "St. Petersburg, Russia      19\n",
       "Bogotá, Colombia            18\n",
       "Udaipur, India              18\n",
       "Moscow, Russia              15\n",
       "Dhaka, Bangladesh           15\n",
       "Santiago, Chile             13\n",
       "Jakarta, Indonesia          11\n",
       "Montevideo, Uruguay          9\n",
       "Cordoba, Argentina           8\n",
       "Lima, Peru                   8\n",
       "Medellín, Colombia           6\n",
       "Calcutta, India              6\n",
       "Rosario, Argentina           5\n",
       "Valparaíso, Chile            3\n",
       "Lençóis, Brazil              3\n",
       "Salvador, Brazil             3\n",
       "Buenos Aires, Argentina      3\n",
       "Bariloche, Argentina         3\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'] = df['post'].map(get_location)\n",
    "location_list = df['location'].value_counts().iloc[:-4]\n",
    "location_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all likes in thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likes_num(x):\n",
    "    match = re.findall('k',x)\n",
    "    if len(match) == 0:\n",
    "        return int(float(x.rstrip('m'))*1000)\n",
    "    else: \n",
    "        return int(float(x.rstrip('k')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>post</th>\n",
       "      <th>no_likes</th>\n",
       "      <th>no_comments</th>\n",
       "      <th>image_labels</th>\n",
       "      <th>image_text</th>\n",
       "      <th>web_entities</th>\n",
       "      <th>location</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...</td>\n",
       "      <td>“I didn’t get accepted into any of the univers...</td>\n",
       "      <td>493.6k</td>\n",
       "      <td>3,806</td>\n",
       "      <td>footwear fashion accessory jeans shoulder shoe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casa Adela Brandon Stanton New York City Bingh...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...</td>\n",
       "      <td>\"I'm trying to live my life without conflict s...</td>\n",
       "      <td>268k</td>\n",
       "      <td>2,714</td>\n",
       "      <td>sitting vehicle temple headgear street health ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brandon Stanton New York City India Black Pant...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/158...</td>\n",
       "      <td>“I was a full time housewife. I kept mostly to...</td>\n",
       "      <td>411.7k</td>\n",
       "      <td>4,103</td>\n",
       "      <td>woman facial expression lady smile snapshot gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Felines of New York: A Glimpse I...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/053...</td>\n",
       "      <td>\"I don't know how old I am.\" (Mumbai, India)</td>\n",
       "      <td>451.4k</td>\n",
       "      <td>5,207</td>\n",
       "      <td>face facial expression yellow head smile eye t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India Humans of New York Humans of New York: S...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/808...</td>\n",
       "      <td>“I resented my mother for the longest time. Sh...</td>\n",
       "      <td>237.2k</td>\n",
       "      <td>1,201</td>\n",
       "      <td>hand finger nail arm jewellery ring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Sadhana H. Varshney Mumbai Human...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>237000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...   \n",
       "1  https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...   \n",
       "2  https://instagram.fftw1-1.fna.fbcdn.net/vp/158...   \n",
       "3  https://instagram.fftw1-1.fna.fbcdn.net/vp/053...   \n",
       "4  https://instagram.fftw1-1.fna.fbcdn.net/vp/808...   \n",
       "\n",
       "                                                post no_likes no_comments  \\\n",
       "0  “I didn’t get accepted into any of the univers...   493.6k       3,806   \n",
       "1  \"I'm trying to live my life without conflict s...     268k       2,714   \n",
       "2  “I was a full time housewife. I kept mostly to...   411.7k       4,103   \n",
       "3       \"I don't know how old I am.\" (Mumbai, India)   451.4k       5,207   \n",
       "4  “I resented my mother for the longest time. Sh...   237.2k       1,201   \n",
       "\n",
       "                                        image_labels image_text  \\\n",
       "0  footwear fashion accessory jeans shoulder shoe...        NaN   \n",
       "1  sitting vehicle temple headgear street health ...        NaN   \n",
       "2  woman facial expression lady smile snapshot gi...        NaN   \n",
       "3  face facial expression yellow head smile eye t...        NaN   \n",
       "4                hand finger nail arm jewellery ring        NaN   \n",
       "\n",
       "                                        web_entities       location   likes  \n",
       "0  Casa Adela Brandon Stanton New York City Bingh...  Mumbai, India  493000  \n",
       "1  Brandon Stanton New York City India Black Pant...  Mumbai, India  268000  \n",
       "2  New York City Felines of New York: A Glimpse I...  Mumbai, India  411000  \n",
       "3  India Humans of New York Humans of New York: S...  Mumbai, India  451000  \n",
       "4  New York City Sadhana H. Varshney Mumbai Human...  Mumbai, India  237000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['likes'] = df['no_likes'].map(lambda x: likes_num(x))*1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn comments into numbers so we can visualize variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comments_num(x):\n",
    "    if \",\" in x:\n",
    "        x = x.replace(',','')\n",
    "        return int(float(x))\n",
    "    elif \"k\" in x:\n",
    "        x = x.rstrip(\"k\")\n",
    "        return int(float(x)*1000)\n",
    "    else:\n",
    "        return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>post</th>\n",
       "      <th>no_likes</th>\n",
       "      <th>no_comments</th>\n",
       "      <th>image_labels</th>\n",
       "      <th>image_text</th>\n",
       "      <th>web_entities</th>\n",
       "      <th>location</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...</td>\n",
       "      <td>“I didn’t get accepted into any of the univers...</td>\n",
       "      <td>493.6k</td>\n",
       "      <td>3,806</td>\n",
       "      <td>footwear fashion accessory jeans shoulder shoe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Casa Adela Brandon Stanton New York City Bingh...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>493000</td>\n",
       "      <td>3806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...</td>\n",
       "      <td>\"I'm trying to live my life without conflict s...</td>\n",
       "      <td>268k</td>\n",
       "      <td>2,714</td>\n",
       "      <td>sitting vehicle temple headgear street health ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brandon Stanton New York City India Black Pant...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>268000</td>\n",
       "      <td>2714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/158...</td>\n",
       "      <td>“I was a full time housewife. I kept mostly to...</td>\n",
       "      <td>411.7k</td>\n",
       "      <td>4,103</td>\n",
       "      <td>woman facial expression lady smile snapshot gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Felines of New York: A Glimpse I...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>411000</td>\n",
       "      <td>4103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/053...</td>\n",
       "      <td>\"I don't know how old I am.\" (Mumbai, India)</td>\n",
       "      <td>451.4k</td>\n",
       "      <td>5,207</td>\n",
       "      <td>face facial expression yellow head smile eye t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India Humans of New York Humans of New York: S...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>451000</td>\n",
       "      <td>5207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://instagram.fftw1-1.fna.fbcdn.net/vp/808...</td>\n",
       "      <td>“I resented my mother for the longest time. Sh...</td>\n",
       "      <td>237.2k</td>\n",
       "      <td>1,201</td>\n",
       "      <td>hand finger nail arm jewellery ring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Sadhana H. Varshney Mumbai Human...</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>237000</td>\n",
       "      <td>1201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0  https://instagram.fftw1-1.fna.fbcdn.net/vp/e49...   \n",
       "1  https://instagram.fftw1-1.fna.fbcdn.net/vp/2ea...   \n",
       "2  https://instagram.fftw1-1.fna.fbcdn.net/vp/158...   \n",
       "3  https://instagram.fftw1-1.fna.fbcdn.net/vp/053...   \n",
       "4  https://instagram.fftw1-1.fna.fbcdn.net/vp/808...   \n",
       "\n",
       "                                                post no_likes no_comments  \\\n",
       "0  “I didn’t get accepted into any of the univers...   493.6k       3,806   \n",
       "1  \"I'm trying to live my life without conflict s...     268k       2,714   \n",
       "2  “I was a full time housewife. I kept mostly to...   411.7k       4,103   \n",
       "3       \"I don't know how old I am.\" (Mumbai, India)   451.4k       5,207   \n",
       "4  “I resented my mother for the longest time. Sh...   237.2k       1,201   \n",
       "\n",
       "                                        image_labels image_text  \\\n",
       "0  footwear fashion accessory jeans shoulder shoe...        NaN   \n",
       "1  sitting vehicle temple headgear street health ...        NaN   \n",
       "2  woman facial expression lady smile snapshot gi...        NaN   \n",
       "3  face facial expression yellow head smile eye t...        NaN   \n",
       "4                hand finger nail arm jewellery ring        NaN   \n",
       "\n",
       "                                        web_entities       location   likes  \\\n",
       "0  Casa Adela Brandon Stanton New York City Bingh...  Mumbai, India  493000   \n",
       "1  Brandon Stanton New York City India Black Pant...  Mumbai, India  268000   \n",
       "2  New York City Felines of New York: A Glimpse I...  Mumbai, India  411000   \n",
       "3  India Humans of New York Humans of New York: S...  Mumbai, India  451000   \n",
       "4  New York City Sadhana H. Varshney Mumbai Human...  Mumbai, India  237000   \n",
       "\n",
       "   comments  \n",
       "0      3806  \n",
       "1      2714  \n",
       "2      4103  \n",
       "3      5207  \n",
       "4      1201  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'] = df['no_comments'].map(lambda x: comments_num(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    267\n",
       "0    205\n",
       "2     85\n",
       "3     28\n",
       "4      8\n",
       "6      3\n",
       "9      1\n",
       "Name: likes_decile, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['likes_decile'] = pd.cut(df['likes'],10, labels = False) # even spacing as opposed to qcut \n",
    "# 1 represents fewest likes\n",
    "df['likes_decile'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146000.0, 252000.0]     267\n",
       "(38940.0, 146000.0]      205\n",
       "(252000.0, 358000.0]      85\n",
       "(358000.0, 464000.0]      28\n",
       "(464000.0, 570000.0]       8\n",
       "(676000.0, 782000.0]       3\n",
       "(994000.0, 1100000.0]      1\n",
       "(888000.0, 994000.0]       0\n",
       "(782000.0, 888000.0]       0\n",
       "(570000.0, 676000.0]       0\n",
       "Name: likes, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bin values\n",
    "pd.cut(df['likes'],10).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations of where posts are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_values = location_list.values\n",
    "type(ll_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Make a data frame with dots to show on the map\n",
    "data = pd.DataFrame({\n",
    "   'lon':[40, 19, 27, -23.5, -23, 60, 5, 24.5, 56, 24, -33, -6, -35, 38, -12, 6, 22.5,\n",
    "         -33, -41.5, -12.5, -13, -35, -41],\n",
    "   'lat':[-74, 73, 76, -47, -43, 30, -74, 74, 38, 90, -71, 107, -56, -5, -77, -75.5, 88,\n",
    "         -61, -71, -41, -38.5, -58, -71],\n",
    "   'name':['New York City', 'Mumbai', 'Jaipur', 'Sao Paolo', 'Rio de Janeiro', 'St Petersburg',\n",
    "          'Bogota', 'Udaipur', 'Moscow', 'Dhaka', 'Santiago', 'Jakarta', 'Montevideo',\n",
    "          'Cordova', 'Lima', 'Medellin', 'Calcutta', 'Rosario', 'Valparasio', 'Lencois',\n",
    "          'Salvador', 'Buenos Aires', 'Bariloche'],\n",
    "   'value':[100, 29, 27, 25, 20, 19, 18, 18, 15, 15, 13, 11, 9, 8, 8, 6, 6, 5, 3, 3, 3, 3, 3]\n",
    "})\n",
    "data\n",
    " \n",
    "# Make an empty map\n",
    "m = folium.Map(location=[20,0], tiles=\"Mapbox Bright\", zoom_start=2)\n",
    " \n",
    "# I can add marker one by one on the map\n",
    "for i in range(0,len(data)):\n",
    "    folium.Circle(\n",
    "      location=[data.iloc[i]['lon'], data.iloc[i]['lat']],\n",
    "      popup=data.iloc[i]['name'],\n",
    "      radius=data.iloc[i]['value']*10000,\n",
    "      color='crimson',\n",
    "      fill=True,\n",
    "      fill_color='crimson'\n",
    "   ).add_to(m)\n",
    " \n",
    "# Save as html\n",
    "m.save('mymap.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math as m\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = re.sub(r'[^\\w\\s]*', '', doc) \n",
    "    doc = re.sub(r'[\\s]+', ' ', doc)\n",
    "    doc = doc.lower().strip()\n",
    "    return doc\n",
    "\n",
    "df['words'] = df['post'].map(lambda x: clean(x).split())\n",
    "clean_docs = df['words'].tolist()\n",
    "# clean_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords - probably want to add locations to the list because those still show up in words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stops = list(stopwords.words('english'))\n",
    "addtl_sw = ['one','got',\"im\",\"ive\",\"id\",'even','like','going','dont','didnt','never','every','always','get'\n",
    "            ,'people','time','want','years','told','feel','life', 'with', 'without', 'think', 'things',\n",
    "           'friends', 'would', 'could', 'us', 'really', 'good', 'i', 'first', 'said']\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    sw_doc = list()\n",
    "    for token in doc:\n",
    "        if not token in stops and not token in addtl_sw:\n",
    "            sw_doc.append(token)\n",
    "    return sw_doc\n",
    "            \n",
    "df['tokens'] = df['words'].map(lambda x: remove_stopwords(x))\n",
    "sw_token_docs = df['tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging, gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def get_corpus(tokens):\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 14:10:16,691 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-01 14:10:16,776 : INFO : built Dictionary(5342 unique tokens: [u'inning', u'wetlands', u'exams', u'todays', u'chile']...) from 597 documents (total 33751 corpus positions)\n",
      "2018-03-01 14:10:17,150 : INFO : using symmetric alpha at 0.1\n",
      "2018-03-01 14:10:17,151 : INFO : using symmetric eta at 0.1\n",
      "2018-03-01 14:10:17,154 : INFO : using serial LDA version on this node\n",
      "2018-03-01 14:10:17,557 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 597 documents, updating model once every 597 documents, evaluating perplexity every 597 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-03-01 14:10:20,425 : INFO : -11.023 per-word bound, 2081.3 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:20,426 : INFO : PROGRESS: pass 0, at document #597/597\n",
      "2018-03-01 14:10:22,129 : INFO : topic #3 (0.100): 0.007*\"make\" + 0.005*\"lost\" + 0.005*\"day\" + 0.004*\"know\" + 0.004*\"girl\" + 0.004*\"go\" + 0.004*\"started\" + 0.004*\"trying\" + 0.004*\"great\" + 0.004*\"home\"\n",
      "2018-03-01 14:10:22,130 : INFO : topic #5 (0.100): 0.006*\"something\" + 0.005*\"hard\" + 0.005*\"know\" + 0.005*\"day\" + 0.005*\"two\" + 0.004*\"make\" + 0.004*\"cancer\" + 0.004*\"new\" + 0.004*\"go\" + 0.004*\"work\"\n",
      "2018-03-01 14:10:22,131 : INFO : topic #1 (0.100): 0.006*\"much\" + 0.006*\"ill\" + 0.005*\"day\" + 0.004*\"still\" + 0.004*\"thought\" + 0.004*\"know\" + 0.004*\"go\" + 0.004*\"cant\" + 0.004*\"back\" + 0.004*\"felt\"\n",
      "2018-03-01 14:10:22,133 : INFO : topic #9 (0.100): 0.007*\"lot\" + 0.006*\"cant\" + 0.005*\"take\" + 0.005*\"make\" + 0.004*\"theyre\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"day\" + 0.004*\"last\" + 0.004*\"mother\"\n",
      "2018-03-01 14:10:22,135 : INFO : topic #8 (0.100): 0.005*\"much\" + 0.005*\"back\" + 0.005*\"went\" + 0.005*\"new\" + 0.004*\"shes\" + 0.004*\"school\" + 0.004*\"everything\" + 0.004*\"go\" + 0.004*\"husband\" + 0.004*\"left\"\n",
      "2018-03-01 14:10:22,136 : INFO : topic diff=5.306594, rho=1.000000\n",
      "2018-03-01 14:10:23,885 : INFO : -8.382 per-word bound, 333.7 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:23,886 : INFO : PROGRESS: pass 1, at document #597/597\n",
      "2018-03-01 14:10:25,144 : INFO : topic #8 (0.100): 0.005*\"much\" + 0.005*\"went\" + 0.005*\"back\" + 0.005*\"new\" + 0.004*\"shes\" + 0.004*\"brazil\" + 0.004*\"go\" + 0.004*\"school\" + 0.004*\"everything\" + 0.004*\"left\"\n",
      "2018-03-01 14:10:25,146 : INFO : topic #9 (0.100): 0.008*\"lot\" + 0.006*\"cant\" + 0.006*\"take\" + 0.005*\"theyre\" + 0.004*\"last\" + 0.004*\"make\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"day\" + 0.004*\"thing\"\n",
      "2018-03-01 14:10:25,147 : INFO : topic #3 (0.100): 0.007*\"make\" + 0.005*\"lost\" + 0.005*\"know\" + 0.005*\"girl\" + 0.005*\"day\" + 0.004*\"go\" + 0.004*\"made\" + 0.004*\"great\" + 0.004*\"started\" + 0.004*\"trying\"\n",
      "2018-03-01 14:10:25,149 : INFO : topic #0 (0.100): 0.006*\"together\" + 0.006*\"shes\" + 0.006*\"lot\" + 0.005*\"started\" + 0.005*\"mom\" + 0.005*\"school\" + 0.005*\"dogs\" + 0.005*\"home\" + 0.005*\"parents\" + 0.005*\"day\"\n",
      "2018-03-01 14:10:25,151 : INFO : topic #6 (0.100): 0.011*\"started\" + 0.006*\"school\" + 0.006*\"came\" + 0.006*\"couldnt\" + 0.006*\"work\" + 0.005*\"mom\" + 0.005*\"made\" + 0.005*\"night\" + 0.005*\"house\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:25,152 : INFO : topic diff=0.694325, rho=0.577350\n",
      "2018-03-01 14:10:26,696 : INFO : -8.131 per-word bound, 280.3 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:26,697 : INFO : PROGRESS: pass 2, at document #597/597\n",
      "2018-03-01 14:10:27,467 : INFO : topic #2 (0.100): 0.006*\"know\" + 0.006*\"school\" + 0.005*\"much\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"city\" + 0.004*\"anything\" + 0.004*\"home\" + 0.004*\"left\" + 0.004*\"tried\"\n",
      "2018-03-01 14:10:27,468 : INFO : topic #0 (0.100): 0.006*\"shes\" + 0.006*\"together\" + 0.006*\"lot\" + 0.005*\"started\" + 0.005*\"day\" + 0.005*\"dogs\" + 0.005*\"school\" + 0.005*\"came\" + 0.005*\"mom\" + 0.005*\"home\"\n",
      "2018-03-01 14:10:27,469 : INFO : topic #6 (0.100): 0.011*\"started\" + 0.007*\"school\" + 0.006*\"came\" + 0.006*\"work\" + 0.006*\"couldnt\" + 0.006*\"made\" + 0.005*\"mom\" + 0.005*\"children\" + 0.005*\"house\" + 0.005*\"night\"\n",
      "2018-03-01 14:10:27,470 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"cant\" + 0.006*\"take\" + 0.005*\"theyre\" + 0.005*\"last\" + 0.004*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"make\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:27,472 : INFO : topic #4 (0.100): 0.007*\"work\" + 0.007*\"go\" + 0.005*\"family\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"lot\" + 0.005*\"wasnt\" + 0.004*\"away\" + 0.004*\"much\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:27,474 : INFO : topic diff=0.500831, rho=0.500000\n",
      "2018-03-01 14:10:29,313 : INFO : -8.027 per-word bound, 260.9 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:29,314 : INFO : PROGRESS: pass 3, at document #597/597\n",
      "2018-03-01 14:10:30,394 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"came\" + 0.004*\"getting\" + 0.004*\"know\" + 0.004*\"night\" + 0.004*\"son\" + 0.004*\"job\" + 0.003*\"see\"\n",
      "2018-03-01 14:10:30,395 : INFO : topic #5 (0.100): 0.007*\"cancer\" + 0.007*\"make\" + 0.006*\"something\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\" + 0.005*\"child\" + 0.005*\"know\"\n",
      "2018-03-01 14:10:30,397 : INFO : topic #3 (0.100): 0.007*\"make\" + 0.006*\"know\" + 0.005*\"lost\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"go\" + 0.005*\"day\" + 0.004*\"great\" + 0.004*\"trying\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:30,398 : INFO : topic #4 (0.100): 0.007*\"work\" + 0.007*\"go\" + 0.005*\"family\" + 0.005*\"back\" + 0.005*\"know\" + 0.005*\"lot\" + 0.005*\"wasnt\" + 0.005*\"away\" + 0.004*\"much\" + 0.004*\"love\"\n",
      "2018-03-01 14:10:30,400 : INFO : topic #1 (0.100): 0.007*\"ill\" + 0.007*\"much\" + 0.005*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"day\" + 0.004*\"india\" + 0.004*\"still\" + 0.004*\"live\" + 0.004*\"parents\"\n",
      "2018-03-01 14:10:30,402 : INFO : topic diff=0.360977, rho=0.447214\n",
      "2018-03-01 14:10:31,925 : INFO : -7.977 per-word bound, 251.9 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:31,926 : INFO : PROGRESS: pass 4, at document #597/597\n",
      "2018-03-01 14:10:32,701 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:32,702 : INFO : topic #1 (0.100): 0.007*\"ill\" + 0.007*\"much\" + 0.005*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"day\" + 0.005*\"india\" + 0.004*\"still\" + 0.004*\"live\" + 0.004*\"parents\"\n",
      "2018-03-01 14:10:32,704 : INFO : topic #3 (0.100): 0.007*\"make\" + 0.006*\"know\" + 0.005*\"lost\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"go\" + 0.005*\"day\" + 0.004*\"great\" + 0.004*\"trying\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:32,705 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.008*\"school\" + 0.007*\"came\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"couldnt\" + 0.005*\"children\" + 0.005*\"house\" + 0.005*\"mom\" + 0.005*\"night\"\n",
      "2018-03-01 14:10:32,707 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"came\" + 0.004*\"getting\" + 0.004*\"son\" + 0.004*\"night\" + 0.004*\"know\" + 0.004*\"job\" + 0.003*\"see\"\n",
      "2018-03-01 14:10:32,708 : INFO : topic diff=0.260450, rho=0.408248\n",
      "2018-03-01 14:10:34,252 : INFO : -7.949 per-word bound, 247.2 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:34,253 : INFO : PROGRESS: pass 5, at document #597/597\n",
      "2018-03-01 14:10:35,011 : INFO : topic #3 (0.100): 0.007*\"make\" + 0.006*\"know\" + 0.006*\"lost\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"go\" + 0.005*\"day\" + 0.004*\"trying\" + 0.004*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:35,013 : INFO : topic #8 (0.100): 0.006*\"much\" + 0.006*\"went\" + 0.005*\"brazil\" + 0.005*\"back\" + 0.005*\"go\" + 0.005*\"new\" + 0.004*\"school\" + 0.004*\"home\" + 0.004*\"everything\" + 0.004*\"wasnt\"\n",
      "2018-03-01 14:10:35,014 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.005*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"day\" + 0.005*\"india\" + 0.004*\"still\" + 0.004*\"live\" + 0.004*\"mother\"\n",
      "2018-03-01 14:10:35,016 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"city\" + 0.005*\"anything\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"college\" + 0.004*\"tried\" + 0.004*\"home\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 14:10:35,018 : INFO : topic #5 (0.100): 0.007*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"something\" + 0.006*\"stories\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"child\" + 0.005*\"money\" + 0.005*\"help\"\n",
      "2018-03-01 14:10:35,020 : INFO : topic diff=0.187869, rho=0.377964\n",
      "2018-03-01 14:10:36,471 : INFO : -7.933 per-word bound, 244.4 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:36,472 : INFO : PROGRESS: pass 6, at document #597/597\n",
      "2018-03-01 14:10:37,173 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"day\" + 0.005*\"india\" + 0.004*\"live\" + 0.004*\"still\" + 0.004*\"mother\"\n",
      "2018-03-01 14:10:37,174 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"child\" + 0.005*\"help\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:37,175 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:37,176 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"college\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"work\"\n",
      "2018-03-01 14:10:37,178 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"came\" + 0.004*\"son\" + 0.004*\"getting\" + 0.004*\"night\" + 0.004*\"job\" + 0.004*\"know\" + 0.004*\"see\"\n",
      "2018-03-01 14:10:37,179 : INFO : topic diff=0.136210, rho=0.353553\n",
      "2018-03-01 14:10:38,611 : INFO : -7.923 per-word bound, 242.8 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:38,612 : INFO : PROGRESS: pass 7, at document #597/597\n",
      "2018-03-01 14:10:39,302 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.005*\"day\" + 0.005*\"child\" + 0.005*\"go\" + 0.005*\"help\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:39,303 : INFO : topic #3 (0.100): 0.008*\"make\" + 0.006*\"know\" + 0.006*\"lost\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"go\" + 0.005*\"trying\" + 0.005*\"day\" + 0.004*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:39,304 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"family\" + 0.005*\"wasnt\" + 0.005*\"lot\" + 0.005*\"know\" + 0.005*\"back\" + 0.005*\"away\" + 0.005*\"love\" + 0.004*\"made\"\n",
      "2018-03-01 14:10:39,305 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"college\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"work\"\n",
      "2018-03-01 14:10:39,307 : INFO : topic #8 (0.100): 0.006*\"much\" + 0.006*\"went\" + 0.006*\"brazil\" + 0.005*\"go\" + 0.005*\"new\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"home\" + 0.004*\"wasnt\" + 0.004*\"everything\"\n",
      "2018-03-01 14:10:39,308 : INFO : topic diff=0.099520, rho=0.333333\n",
      "2018-03-01 14:10:40,758 : INFO : -7.917 per-word bound, 241.7 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:40,759 : INFO : PROGRESS: pass 8, at document #597/597\n",
      "2018-03-01 14:10:41,454 : INFO : topic #8 (0.100): 0.006*\"much\" + 0.006*\"brazil\" + 0.006*\"went\" + 0.005*\"go\" + 0.005*\"new\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"home\" + 0.004*\"wasnt\" + 0.004*\"everything\"\n",
      "2018-03-01 14:10:41,456 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"came\" + 0.004*\"son\" + 0.004*\"getting\" + 0.004*\"job\" + 0.004*\"night\" + 0.004*\"know\" + 0.004*\"see\"\n",
      "2018-03-01 14:10:41,457 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"work\"\n",
      "2018-03-01 14:10:41,458 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:41,460 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.008*\"school\" + 0.007*\"came\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"children\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"mom\"\n",
      "2018-03-01 14:10:41,462 : INFO : topic diff=0.073610, rho=0.316228\n",
      "2018-03-01 14:10:42,896 : INFO : -7.913 per-word bound, 240.9 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:42,897 : INFO : PROGRESS: pass 9, at document #597/597\n",
      "2018-03-01 14:10:43,587 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.005*\"help\" + 0.005*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:43,588 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:43,590 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"family\" + 0.005*\"wasnt\" + 0.005*\"lot\" + 0.005*\"know\" + 0.005*\"away\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:43,591 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.008*\"school\" + 0.007*\"came\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"children\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"mom\"\n",
      "2018-03-01 14:10:43,593 : INFO : topic #8 (0.100): 0.006*\"much\" + 0.006*\"brazil\" + 0.006*\"went\" + 0.005*\"go\" + 0.005*\"new\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"home\" + 0.004*\"wasnt\" + 0.004*\"everything\"\n",
      "2018-03-01 14:10:43,594 : INFO : topic diff=0.055331, rho=0.301511\n",
      "2018-03-01 14:10:45,046 : INFO : -7.909 per-word bound, 240.4 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:45,047 : INFO : PROGRESS: pass 10, at document #597/597\n",
      "2018-03-01 14:10:45,737 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.005*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:45,739 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.009*\"school\" + 0.007*\"came\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"children\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"morning\"\n",
      "2018-03-01 14:10:45,740 : INFO : topic #0 (0.100): 0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.005*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\"\n",
      "2018-03-01 14:10:45,741 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"work\"\n",
      "2018-03-01 14:10:45,743 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"know\" + 0.005*\"lot\" + 0.005*\"away\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:45,744 : INFO : topic diff=0.042215, rho=0.288675\n",
      "2018-03-01 14:10:47,180 : INFO : -7.907 per-word bound, 240.0 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:47,181 : INFO : PROGRESS: pass 11, at document #597/597\n",
      "2018-03-01 14:10:47,866 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:47,867 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"home\"\n",
      "2018-03-01 14:10:47,868 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"know\" + 0.005*\"lot\" + 0.005*\"away\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:47,870 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"day\" + 0.004*\"still\" + 0.004*\"mother\"\n",
      "2018-03-01 14:10:47,871 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.005*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:47,873 : INFO : topic diff=0.032691, rho=0.277350\n",
      "2018-03-01 14:10:49,314 : INFO : -7.905 per-word bound, 239.6 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 14:10:49,315 : INFO : PROGRESS: pass 12, at document #597/597\n",
      "2018-03-01 14:10:50,011 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"day\" + 0.005*\"still\" + 0.005*\"mother\"\n",
      "2018-03-01 14:10:50,013 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"home\"\n",
      "2018-03-01 14:10:50,014 : INFO : topic #8 (0.100): 0.006*\"much\" + 0.006*\"brazil\" + 0.006*\"went\" + 0.005*\"go\" + 0.005*\"new\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"home\" + 0.004*\"wasnt\" + 0.004*\"everything\"\n",
      "2018-03-01 14:10:50,016 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:50,017 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.009*\"school\" + 0.007*\"came\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"children\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"morning\"\n",
      "2018-03-01 14:10:50,019 : INFO : topic diff=0.025711, rho=0.267261\n",
      "2018-03-01 14:10:51,453 : INFO : -7.903 per-word bound, 239.4 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:51,454 : INFO : PROGRESS: pass 13, at document #597/597\n",
      "2018-03-01 14:10:52,139 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"day\"\n",
      "2018-03-01 14:10:52,140 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"wasnt\" + 0.005*\"know\" + 0.005*\"family\" + 0.005*\"lot\" + 0.005*\"away\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:52,142 : INFO : topic #0 (0.100): 0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.005*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\"\n",
      "2018-03-01 14:10:52,144 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:52,146 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:52,148 : INFO : topic diff=0.020547, rho=0.258199\n",
      "2018-03-01 14:10:53,588 : INFO : -7.902 per-word bound, 239.1 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:53,589 : INFO : PROGRESS: pass 14, at document #597/597\n",
      "2018-03-01 14:10:54,275 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"know\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"lot\" + 0.005*\"away\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:10:54,276 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"still\"\n",
      "2018-03-01 14:10:54,278 : INFO : topic #0 (0.100): 0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\"\n",
      "2018-03-01 14:10:54,279 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.006*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:54,281 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"home\"\n",
      "2018-03-01 14:10:54,282 : INFO : topic diff=0.016658, rho=0.250000\n",
      "2018-03-01 14:10:55,718 : INFO : -7.900 per-word bound, 238.9 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:55,719 : INFO : PROGRESS: pass 15, at document #597/597\n",
      "2018-03-01 14:10:56,404 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"still\"\n",
      "2018-03-01 14:10:56,405 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.009*\"school\" + 0.007*\"came\" + 0.006*\"children\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"morning\"\n",
      "2018-03-01 14:10:56,406 : INFO : topic #0 (0.100): 0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\"\n",
      "2018-03-01 14:10:56,408 : INFO : topic #3 (0.100): 0.008*\"make\" + 0.006*\"lost\" + 0.006*\"know\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"trying\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:56,410 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"son\" + 0.004*\"came\" + 0.004*\"getting\" + 0.004*\"job\" + 0.004*\"night\" + 0.004*\"along\" + 0.004*\"know\"\n",
      "2018-03-01 14:10:56,412 : INFO : topic diff=0.013692, rho=0.242536\n",
      "2018-03-01 14:10:57,866 : INFO : -7.899 per-word bound, 238.8 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:57,867 : INFO : PROGRESS: pass 16, at document #597/597\n",
      "2018-03-01 14:10:58,548 : INFO : topic #2 (0.100): 0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"home\"\n",
      "2018-03-01 14:10:58,550 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.007*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"day\" + 0.005*\"go\" + 0.005*\"money\"\n",
      "2018-03-01 14:10:58,551 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"went\"\n",
      "2018-03-01 14:10:58,552 : INFO : topic #0 (0.100): 0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\"\n",
      "2018-03-01 14:10:58,554 : INFO : topic #3 (0.100): 0.008*\"make\" + 0.006*\"lost\" + 0.006*\"know\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"trying\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:10:58,555 : INFO : topic diff=0.011399, rho=0.235702\n",
      "2018-03-01 14:10:59,967 : INFO : -7.899 per-word bound, 238.6 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:10:59,968 : INFO : PROGRESS: pass 17, at document #597/597\n",
      "2018-03-01 14:11:00,670 : INFO : topic #6 (0.100): 0.012*\"started\" + 0.009*\"school\" + 0.007*\"came\" + 0.007*\"children\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"morning\"\n",
      "2018-03-01 14:11:00,671 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"went\"\n",
      "2018-03-01 14:11:00,672 : INFO : topic #7 (0.100): 0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"son\" + 0.004*\"came\" + 0.004*\"getting\" + 0.004*\"job\" + 0.004*\"night\" + 0.004*\"along\" + 0.004*\"see\"\n",
      "2018-03-01 14:11:00,673 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"india\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"still\"\n",
      "2018-03-01 14:11:00,675 : INFO : topic #8 (0.100): 0.006*\"brazil\" + 0.006*\"much\" + 0.006*\"went\" + 0.005*\"new\" + 0.005*\"go\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"wasnt\" + 0.004*\"home\" + 0.004*\"everything\"\n",
      "2018-03-01 14:11:00,677 : INFO : topic diff=0.009615, rho=0.229416\n",
      "2018-03-01 14:11:02,102 : INFO : -7.898 per-word bound, 238.5 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:11:02,103 : INFO : PROGRESS: pass 18, at document #597/597\n",
      "2018-03-01 14:11:02,777 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"know\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"away\" + 0.005*\"lot\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-01 14:11:02,778 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"mother\" + 0.005*\"india\" + 0.005*\"everything\" + 0.005*\"still\"\n",
      "2018-03-01 14:11:02,780 : INFO : topic #9 (0.100): 0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"everything\" + 0.004*\"went\"\n",
      "2018-03-01 14:11:02,781 : INFO : topic #3 (0.100): 0.008*\"make\" + 0.006*\"lost\" + 0.006*\"know\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"trying\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:11:02,783 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.007*\"cancers\" + 0.006*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"money\"\n",
      "2018-03-01 14:11:02,784 : INFO : topic diff=0.008204, rho=0.223607\n",
      "2018-03-01 14:11:04,203 : INFO : -7.897 per-word bound, 238.4 perplexity estimate based on a held-out corpus of 597 documents with 33751 words\n",
      "2018-03-01 14:11:04,204 : INFO : PROGRESS: pass 19, at document #597/597\n",
      "2018-03-01 14:11:04,896 : INFO : topic #4 (0.100): 0.008*\"work\" + 0.007*\"go\" + 0.005*\"know\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"away\" + 0.005*\"lot\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\"\n",
      "2018-03-01 14:11:04,897 : INFO : topic #8 (0.100): 0.006*\"brazil\" + 0.006*\"much\" + 0.006*\"went\" + 0.005*\"new\" + 0.005*\"go\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"wasnt\" + 0.004*\"home\" + 0.004*\"everything\"\n",
      "2018-03-01 14:11:04,898 : INFO : topic #1 (0.100): 0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"india\" + 0.005*\"still\"\n",
      "2018-03-01 14:11:04,899 : INFO : topic #3 (0.100): 0.008*\"make\" + 0.006*\"lost\" + 0.006*\"know\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"trying\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"great\" + 0.004*\"still\"\n",
      "2018-03-01 14:11:04,901 : INFO : topic #5 (0.100): 0.008*\"cancer\" + 0.007*\"make\" + 0.007*\"cancers\" + 0.007*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"go\" + 0.005*\"money\" + 0.005*\"day\"\n",
      "2018-03-01 14:11:04,903 : INFO : topic diff=0.007077, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(sw_token_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in sw_token_docs]\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10, update_every=1, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #  0 (0, u'0.007*\"shes\" + 0.006*\"dogs\" + 0.006*\"together\" + 0.006*\"day\" + 0.006*\"lot\" + 0.006*\"dog\" + 0.005*\"came\" + 0.005*\"started\" + 0.005*\"married\" + 0.005*\"heart\" + 0.005*\"back\" + 0.005*\"school\" + 0.004*\"parents\" + 0.004*\"everyone\" + 0.004*\"mom\"')\n",
      "Topic #  1 (1, u'0.008*\"ill\" + 0.007*\"much\" + 0.006*\"felt\" + 0.005*\"thought\" + 0.005*\"wanted\" + 0.005*\"live\" + 0.005*\"mother\" + 0.005*\"everything\" + 0.005*\"india\" + 0.005*\"still\" + 0.005*\"day\" + 0.004*\"mom\" + 0.004*\"two\" + 0.004*\"parents\" + 0.004*\"cant\"')\n",
      "Topic #  2 (2, u'0.007*\"know\" + 0.006*\"much\" + 0.006*\"school\" + 0.005*\"college\" + 0.005*\"anything\" + 0.005*\"city\" + 0.005*\"go\" + 0.005*\"everything\" + 0.004*\"tried\" + 0.004*\"home\" + 0.004*\"worked\" + 0.004*\"work\" + 0.004*\"back\" + 0.004*\"couldnt\" + 0.004*\"went\"')\n",
      "Topic #  3 (3, u'0.008*\"make\" + 0.006*\"lost\" + 0.006*\"know\" + 0.005*\"girl\" + 0.005*\"made\" + 0.005*\"trying\" + 0.005*\"go\" + 0.005*\"day\" + 0.005*\"great\" + 0.004*\"still\" + 0.004*\"tried\" + 0.004*\"started\" + 0.004*\"entire\" + 0.004*\"home\" + 0.004*\"try\"')\n",
      "Topic #  4 (4, u'0.008*\"work\" + 0.007*\"go\" + 0.005*\"know\" + 0.005*\"wasnt\" + 0.005*\"family\" + 0.005*\"away\" + 0.005*\"lot\" + 0.005*\"love\" + 0.005*\"back\" + 0.004*\"day\" + 0.004*\"money\" + 0.004*\"made\" + 0.004*\"hard\" + 0.004*\"much\" + 0.004*\"ago\"')\n",
      "Topic #  5 (5, u'0.008*\"cancer\" + 0.007*\"make\" + 0.007*\"cancers\" + 0.007*\"stories\" + 0.006*\"something\" + 0.006*\"help\" + 0.006*\"child\" + 0.005*\"go\" + 0.005*\"money\" + 0.005*\"day\" + 0.005*\"please\" + 0.005*\"tumor\" + 0.005*\"bio\" + 0.005*\"link\" + 0.005*\"rare\"')\n",
      "Topic #  6 (6, u'0.012*\"started\" + 0.009*\"school\" + 0.007*\"came\" + 0.007*\"children\" + 0.006*\"work\" + 0.006*\"made\" + 0.006*\"couldnt\" + 0.006*\"house\" + 0.005*\"night\" + 0.005*\"morning\" + 0.005*\"mom\" + 0.005*\"still\" + 0.005*\"india\" + 0.004*\"money\" + 0.004*\"husband\"')\n",
      "Topic #  7 (7, u'0.011*\"day\" + 0.005*\"ill\" + 0.005*\"make\" + 0.004*\"son\" + 0.004*\"came\" + 0.004*\"getting\" + 0.004*\"job\" + 0.004*\"night\" + 0.004*\"along\" + 0.004*\"see\" + 0.004*\"weve\" + 0.004*\"working\" + 0.003*\"know\" + 0.003*\"started\" + 0.003*\"love\"')\n",
      "Topic #  8 (8, u'0.006*\"brazil\" + 0.006*\"much\" + 0.006*\"went\" + 0.005*\"new\" + 0.005*\"go\" + 0.005*\"back\" + 0.004*\"school\" + 0.004*\"wasnt\" + 0.004*\"home\" + 0.004*\"everything\" + 0.004*\"rest\" + 0.004*\"wanted\" + 0.004*\"old\" + 0.004*\"lot\" + 0.004*\"getting\"')\n",
      "Topic #  9 (9, u'0.009*\"lot\" + 0.006*\"take\" + 0.006*\"cant\" + 0.006*\"theyre\" + 0.005*\"last\" + 0.005*\"thing\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"went\" + 0.004*\"everything\" + 0.004*\"day\" + 0.004*\"left\" + 0.004*\"together\" + 0.004*\"say\" + 0.004*\"make\"')\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "for i in lda.show_topics(num_topics=10, num_words=15, log=False, formatted=True):\n",
    "    print \"Topic # \", t , i\n",
    "    t = t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "\n",
    "seed(2)\n",
    "all_topics = lda.get_document_topics(corpus, per_word_topics = True)\n",
    "i = 1\n",
    "doc_topics_df = pd.DataFrame(columns = ['Speech No.', 'Doc Topics'])\n",
    "for doc_topics, word_topics, phi_values in all_topics:\n",
    "    temp_df = pd.DataFrame([[i, doc_topics]], columns = ['Speech No.', 'Doc Topics'])\n",
    "    doc_topics_df = doc_topics_df.append(temp_df, ignore_index = True)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(x):\n",
    "    pattern = \"\\(\\d+\\,\"\n",
    "    x = str(x)\n",
    "    matches = re.findall(pattern, x)\n",
    "    list_of_m = []\n",
    "    for m in matches:\n",
    "        m = m.lstrip(\"(\")\n",
    "        m = m.rstrip(\",\")\n",
    "        list_of_m.append(m)\n",
    "    return list_of_m\n",
    "\n",
    "seed(999)\n",
    "doc_topics_df['Topics'] = doc_topics_df['Doc Topics'].map(get_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech No.</th>\n",
       "      <th>Doc Topics</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic 10</th>\n",
       "      <th>Topic 11</th>\n",
       "      <th>Topic 12</th>\n",
       "      <th>Topic 13</th>\n",
       "      <th>Topic 14</th>\n",
       "      <th>Topic 15</th>\n",
       "      <th>Topic 16</th>\n",
       "      <th>Topic 17</th>\n",
       "      <th>Topic 18</th>\n",
       "      <th>Topic 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(1, 0.9894103)]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(0, 0.012501627), (1, 0.8874823), (2, 0.01250...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[(6, 0.98524433)]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[(0, 0.020004926), (1, 0.020005092), (2, 0.020...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[(9, 0.9819977)]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speech No.                                         Doc Topics  \\\n",
       "0          1                                   [(1, 0.9894103)]   \n",
       "1          2  [(0, 0.012501627), (1, 0.8874823), (2, 0.01250...   \n",
       "2          3                                  [(6, 0.98524433)]   \n",
       "3          4  [(0, 0.020004926), (1, 0.020005092), (2, 0.020...   \n",
       "4          5                                   [(9, 0.9819977)]   \n",
       "\n",
       "                           Topics  Topic 0  Topic 1  Topic 2  Topic 3  \\\n",
       "0                             [1]        0        1        0        0   \n",
       "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]        1        1        1        1   \n",
       "2                             [6]        0        0        0        0   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]        1        1        1        1   \n",
       "4                             [9]        0        0        0        0   \n",
       "\n",
       "   Topic 4  Topic 5  Topic 6    ...     Topic 10  Topic 11  Topic 12  \\\n",
       "0        0        0        0    ...            0         0         0   \n",
       "1        1        1        1    ...            0         0         0   \n",
       "2        0        0        1    ...            0         0         0   \n",
       "3        1        1        1    ...            0         0         0   \n",
       "4        0        0        0    ...            0         0         0   \n",
       "\n",
       "   Topic 13  Topic 14  Topic 15  Topic 16  Topic 17  Topic 18  Topic 19  \n",
       "0         0         0         0         0         0         0         0  \n",
       "1         0         0         0         0         0         0         0  \n",
       "2         0         0         0         0         0         0         0  \n",
       "3         0         0         0         0         0         0         0  \n",
       "4         0         0         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_columns(i, lst):\n",
    "    if i in lst:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "seed(999)\n",
    "for j in range(0,20):\n",
    "    doc_topics_df['Topic ' + str(j)] = doc_topics_df['Topics'].apply(lambda x: new_columns(str(j), x))\n",
    "\n",
    "doc_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "documentation": "# Enter Documentation Here...",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
